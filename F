import streamlit as st
import os
import pandas as pd
from datetime import datetime

from azure.ai.contentsafety import ContentSafetyClient
from azure.ai.contentsafety.models import (
    AnalyzeTextOptions,
    AnalyzeTextOutputType,
    TextBlocklist,
    AddOrUpdateTextBlocklistItemsOptions,
    TextBlocklistItem,
    AnalyzeTextCategories,
    TextCategory,
    ShieldPromptAnalysis, # Placeholder, actual class might differ or be part of AnalyzeTextOptions
    AnalyzeTextByBlocklistOptions, # Placeholder if specific options exist
)
from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import HttpResponseError
from azure.identity import DefaultAzureCredential

# Using OpenAI SDK for LLM call - adjust if your Foundry deployment differs
import openai

# For Application Insights logging
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator
from opentelemetry.propagate import set_global_textmap_propagator


# --- Configuration & Clients ---
def load_env_vars():
    """Load environment variables."""
    config = {
        "content_safety_endpoint": os.getenv("AZURE_CONTENT_SAFETY_ENDPOINT"),
        "content_safety_key": os.getenv("AZURE_CONTENT_SAFETY_KEY"),
        "ai_foundry_endpoint": os.getenv("AZURE_AI_FOUNDRY_ENDPOINT"),
        "ai_foundry_key": os.getenv("AZURE_AI_FOUNDRY_KEY"), # May not be needed if using AAD
        "app_insights_connection_string": os.getenv("APPLICATIONINSIGHTS_CONNECTION_STRING"),
    }
    if not all([config["content_safety_endpoint"], config["content_safety_key"], config["ai_foundry_endpoint"], config["app_insights_connection_string"]]):
        st.error("One or more critical environment variables are missing. Please check your setup.")
        st.stop()
    return config

CONFIG = load_env_vars()

# Initialize Azure Clients
try:
    # Content Safety Client
    cs_credential = AzureKeyCredential(CONFIG["content_safety_key"])
    content_safety_client = ContentSafetyClient(CONFIG["content_safety_endpoint"], cs_credential)

    # For LLM Client (using OpenAI SDK as an example for a Foundry deployment)
    # If your Foundry endpoint uses AAD auth, you might need to configure differently
    # For Azure OpenAI, it might be:
    # openai.api_type = "azure"
    # openai.api_base = CONFIG["ai_foundry_endpoint"]
    # openai.api_version = "2024-02-01" # Or your model's API version
    # openai.api_key = CONFIG["ai_foundry_key"] # Or use DefaultAzureCredential
    # For other Foundry models, the SDK/auth might differ.
    # This example assumes a generic OpenAI-compatible endpoint.
    # If your endpoint is a raw HTTP endpoint:
    # Use 'requests' library. This example will proceed with openai for simplicity.
    llm_client = openai.OpenAI(
        base_url=CONFIG["ai_foundry_endpoint"], # Ensure this points to the chat/completions part of your endpoint
        api_key=CONFIG["ai_foundry_key"] if CONFIG["ai_foundry_key"] else "dummy-key-if-not-needed" # Some endpoints might be keyless if secured by VNet/PrivateLink + AAD
    )
    # Example model name - replace with your deployed model name in AI Foundry
    AI_FOUNDRY_DEPLOYMENT_NAME = "gpt-4o-mini" # Or whatever your deployment is named

except Exception as e:
    st.error(f"Error initializing Azure clients: {e}")
    st.stop()


# --- Application Insights Setup ---
def setup_opentelemetry():
    """Sets up OpenTelemetry for Azure Monitor."""
    if CONFIG["app_insights_connection_string"]:
        try:
            trace.set_tracer_provider(TracerProvider())
            tracer = trace.get_tracer(__name__)
            span_processor = BatchSpanProcessor(
                AzureMonitorTraceExporter(connection_string=CONFIG["app_insights_connection_string"])
            )
            trace.get_tracer_provider().add_span_processor(span_processor)
            set_global_textmap_propagator(TraceContextTextMapPropagator()) # Important for context propagation
            st.session_state.tracer = tracer
            print("OpenTelemetry configured for Application Insights.")
            return tracer
        except Exception as e:
            st.warning(f"Could not initialize Application Insights tracer: {e}")
    else:
        st.warning("Application Insights connection string not found. Logging will be to console only.")
    return None

if 'tracer' not in st.session_state:
    st.session_state.tracer = setup_opentelemetry()

def log_to_app_insights(event_name, properties):
    """Logs a custom event to Application Insights if tracer is available."""
    if st.session_state.tracer:
        with st.session_state.tracer.start_as_current_span(event_name) as span:
            for key, value in properties.items():
                span.set_attribute(key, str(value)) # Attributes must be basic types
    else:
        print(f"App Insights not configured. Event: {event_name}, Properties: {properties}")

# --- Content Safety Functions ---

def manage_blocklist(blocklist_name, description="Managed by Streamlit Test App"):
    """Creates or updates a blocklist. Returns the blocklist name."""
    try:
        blocklist = content_safety_client.create_or_update_text_blocklist(
            blocklist_name=blocklist_name,
            options=TextBlocklist(blocklist_name=blocklist_name, description=description)
        )
        log_to_app_insights("BlocklistManagement", {"action": "create_or_update", "blocklist_name": blocklist_name, "status": "success"})
        return blocklist.blocklist_name
    except HttpResponseError as e:
        st.error(f"Failed to create/update blocklist '{blocklist_name}': {e.message}")
        log_to_app_insights("BlocklistManagement", {"action": "create_or_update", "blocklist_name": blocklist_name, "status": "failed", "error": str(e)})
        return None

def add_terms_to_blocklist(blocklist_name, terms_to_add):
    """Adds terms to a given blocklist. terms_to_add is a list of strings."""
    if not terms_to_add:
        return
    try:
        blocklist_items = [TextBlocklistItem(text=term) for term in terms_to_add]
        result = content_safety_client.add_or_update_blocklist_items(
            blocklist_name=blocklist_name,
            options=AddOrUpdateTextBlocklistItemsOptions(blocklist_items=blocklist_items)
        )
        st.success(f"Added {len(result.blocklist_items)} terms to blocklist '{blocklist_name}'.")
        log_to_app_insights("BlocklistManagement", {"action": "add_terms", "blocklist_name": blocklist_name, "term_count": len(terms_to_add), "status": "success"})
    except HttpResponseError as e:
        st.error(f"Failed to add terms to blocklist '{blocklist_name}': {e.message}")
        log_to_app_insights("BlocklistManagement", {"action": "add_terms", "blocklist_name": blocklist_name, "status": "failed", "error": str(e)})

def analyze_text_content_safety(text_to_analyze, blocklist_names=None, halt_on_blocklist=False):
    """Analyzes text using Azure AI Content Safety, optionally with blocklists."""
    request_options = AnalyzeTextOptions(
        text=text_to_analyze,
        categories=[
            TextCategory.HATE, TextCategory.SELF_HARM,
            TextCategory.SEXUAL, TextCategory.VIOLENCE
        ],
        output_type=AnalyzeTextOutputType.SEVERITY_LEVEL # Or FOUR_STEP_CLASSIFICATION
    )
    if blocklist_names:
        request_options.blocklist_names = blocklist_names
        request_options.halt_on_blocklist_hit = halt_on_blocklist

    analysis_result = None
    try:
        response = content_safety_client.analyze_text(request_options)
        analysis_result = {
            "categories_analysis": [
                {"category": cat.category, "severity": cat.severity} for cat in response.categories_analysis
            ] if response.categories_analysis else [],
            "blocklists_match": [
                {"blocklist_name": match.blocklist_name, "hit_text": match.blocklist_item_text}
                for match in response.blocklists_match_results # Adjusted based on potential SDK structure (was blocklists_match)
            ] if response.blocklists_match_results else [], # Changed from blocklists_match
        }
        log_to_app_insights("ContentSafetyAnalysis", {"text_length": len(text_to_analyze), "blocklists_used": bool(blocklist_names), "status": "success"})
    except HttpResponseError as e:
        st.error(f"Content Safety analysis failed: {e.message}")
        analysis_result = {"error": str(e.message)}
        log_to_app_insights("ContentSafetyAnalysis", {"text_length": len(text_to_analyze), "status": "failed", "error": str(e)})
    return analysis_result

def analyze_with_prompt_shield(user_prompt, documents=None):
    """
    Analyzes a user prompt and optional documents for attacks using Prompt Shields.
    Note: The Python SDK for Prompt Shield might evolve.
    This is based on the REST API structure /contentsafety/text:shieldPrompt?api-version=2024-09-01
    The ContentSafetyClient might offer a direct method like `shield_prompt` or it might be integrated
    into `analyze_text` with specific options. For this example, we'll conceptualize its usage.
    Refer to the latest SDK documentation for the exact method.

    As of recent documentation, there isn't a direct `shield_prompt` method in the Python SDK's
    ContentSafetyClient as clearly defined as `analyze_text`.
    It's possible it's integrated or uses a different client/method.
    For now, this function will simulate the expected output structure.
    In a real scenario, you'd replace this with the actual SDK call if available,
    or make a direct REST API call.
    """
    st.warning("Prompt Shield SDK call is conceptual. Verify with the latest azure-ai-contentsafety SDK documentation.")
    log_to_app_insights("PromptShieldAnalysis", {"user_prompt_length": len(user_prompt), "doc_count": len(documents) if documents else 0, "status": "conceptual"})

    # Conceptual structure - replace with actual SDK call
    # Example of what a direct REST call might look like (not executed here)
    # import requests
    # shield_endpoint = f"{CONFIG['content_safety_endpoint']}/contentsafety/text:shieldPrompt?api-version=2024-09-01-preview" # or newer
    # headers = {"Ocp-Apim-Subscription-Key": CONFIG['content_safety_key'], "Content-Type": "application/json"}
    # payload = {"userPrompt": user_prompt}
    # if documents:
    #     payload["documents"] = documents
    # try:
    #     response = requests.post(shield_endpoint, headers=headers, json=payload)
    #     response.raise_for_status()
    #     result = response.json()
    #     return {
    #         "user_prompt_analysis": result.get("userPromptAnalysis", {}),
    #         "documents_analysis": result.get("documentsAnalysis", [])
    #     }
    # except requests.exceptions.RequestException as e:
    #     st.error(f"Prompt Shield REST call failed: {e}")
    #     return {"error": str(e)}

    # Simulated response for UI testing
    return {
        "user_prompt_analysis": {"attack_detected": "Unknown (Conceptual)"},
        "documents_analysis": [{"attack_detected": "Unknown (Conceptual)"} for _ in documents] if documents else []
    }


# --- LLM Call Function ---
def call_ai_foundry_llm(prompt):
    """Calls the deployed LLM in AI Foundry."""
    completion_text = ""
    full_response = None
    try:
        # Example for a chat model using OpenAI SDK
        response = llm_client.chat.completions.create(
            model=AI_FOUNDRY_DEPLOYMENT_NAME,  # Your deployment name
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.7
        )
        completion_text = response.choices[0].message.content.strip()
        full_response = response.model_dump_json(indent=2) # For logging
        log_to_app_insights("LLMCall", {"prompt_length": len(prompt), "completion_length": len(completion_text), "model": AI_FOUNDRY_DEPLOYMENT_NAME, "status": "success"})
    except Exception as e:
        st.error(f"LLM call failed: {e}")
        completion_text = f"Error: {e}"
        log_to_app_insights("LLMCall", {"prompt_length": len(prompt), "model": AI_FOUNDRY_DEPLOYMENT_NAME, "status": "failed", "error": str(e)})
    return prompt, completion_text, full_response

# --- Streamlit UI ---
st.set_page_config(layout="wide")
st.title("üõ°Ô∏è Azure AI Content Safety & Foundry Test UI üß™")
st.markdown(f"Testing against Content Safety Endpoint: `{CONFIG['content_safety_endpoint']}`")
st.markdown(f"Using LLM from AI Foundry Endpoint: `{CONFIG['ai_foundry_endpoint']}` (Model: `{AI_FOUNDRY_DEPLOYMENT_NAME}`)")
st.markdown(f"Logging to Application Insights: `{'Enabled' if st.session_state.tracer else 'Disabled'}`")

# --- Blocklist Management UI ---
with st.sidebar:
    st.header("Blocklist Management")
    blocklist_name_input = st.text_input("Blocklist Name", value="my-test-blocklist")
    if st.button("Create/Ensure Blocklist Exists"):
        created_blocklist_name = manage_blocklist(blocklist_name_input)
        if created_blocklist_name:
            st.session_state.current_blocklist = created_blocklist_name
            st.success(f"Blocklist '{created_blocklist_name}' is ready.")

    if 'current_blocklist' in st.session_state and st.session_state.current_blocklist:
        st.write(f"Current active blocklist: **{st.session_state.current_blocklist}**")
        terms_to_add_str = st.text_area("Terms to add (comma-separated)", placeholder="e.g., unwanted_term1, another_term")
        if st.button("Add Terms to Blocklist"):
            if terms_to_add_str:
                terms = [term.strip() for term in terms_to_add_str.split(',') if term.strip()]
                add_terms_to_blocklist(st.session_state.current_blocklist, terms)
            else:
                st.warning("Please enter terms to add.")
    else:
        st.info("Create or specify a blocklist to add terms.")


col1, col2 = st.columns(2)

with col1:
    st.header("Input Prompt")
    user_prompt = st.text_area("Enter your prompt for the LLM:", height=200, key="user_prompt_input")

    st.subheader("Content Safety Options")
    use_blocklist = st.checkbox("Enable Blocklist Check on Prompt/Completion", value=False)
    selected_blocklist = None
    if use_blocklist:
        if 'current_blocklist' in st.session_state and st.session_state.current_blocklist:
            selected_blocklist = st.session_state.current_blocklist
            st.info(f"Using blocklist: {selected_blocklist}")
        else:
            st.warning("No active blocklist. Create one or disable blocklist check.")
            use_blocklist = False # Disable if no blocklist is active

    use_prompt_shield = st.checkbox("Enable Prompt Shield Analysis (Conceptual)", value=False)
    document_for_shield = ""
    if use_prompt_shield:
        document_for_shield = st.text_area("Enter document text for Prompt Shield (optional):", height=100, key="prompt_shield_doc")

    if st.button("üöÄ Analyze Prompt & Call LLM", type="primary"):
        if not user_prompt.strip():
            st.warning("Please enter a prompt.")
        else:
            st.session_state.results = {} # Clear previous results
            request_timestamp = datetime.utcnow().isoformat() + "Z"

            # 1. Analyze Prompt with Content Safety (Optional - before LLM)
            st.subheader("üìù Prompt Analysis (Content Safety)")
            prompt_cs_results = analyze_text_content_safety(user_prompt, [selected_blocklist] if use_blocklist and selected_blocklist else None)
            st.session_state.results["prompt_cs_analysis"] = prompt_cs_results
            if prompt_cs_results and "error" not in prompt_cs_results:
                st.json(prompt_cs_results)
            elif prompt_cs_results:
                st.error(f"Prompt CS Error: {prompt_cs_results.get('error')}")

            # 2. Analyze Prompt with Prompt Shield (Optional - before LLM)
            if use_prompt_shield:
                st.subheader("üõ°Ô∏è Prompt Shield Analysis (Conceptual)")
                shield_docs = [doc.strip() for doc in document_for_shield.split("\n") if doc.strip()] if document_for_shield else None
                prompt_shield_results = analyze_with_prompt_shield(user_prompt, shield_docs)
                st.session_state.results["prompt_shield_analysis"] = prompt_shield_results
                if prompt_shield_results and "error" not in prompt_shield_results:
                    st.json(prompt_shield_results)
                elif prompt_shield_results:
                    st.error(f"Prompt Shield Error: {prompt_shield_results.get('error')}")


            # 3. Call LLM
            st.subheader("üí¨ LLM Interaction")
            llm_prompt, llm_completion, llm_full_response = call_ai_foundry_llm(user_prompt)
            st.session_state.results["llm_prompt"] = llm_prompt
            st.session_state.results["llm_completion"] = llm_completion
            st.markdown("**LLM Prompt:**")
            st.text(llm_prompt)
            st.markdown("**LLM Completion:**")
            st.text_area("Completion Output", value=llm_completion, height=200, disabled=True, key="llm_output_display")


            # 4. Analyze Completion with Content Safety
            st.subheader("‚úÖ Completion Analysis (Content Safety)")
            if llm_completion and not llm_completion.startswith("Error:"):
                completion_cs_results = analyze_text_content_safety(llm_completion, [selected_blocklist] if use_blocklist and selected_blocklist else None)
                st.session_state.results["completion_cs_analysis"] = completion_cs_results
                if completion_cs_results and "error" not in completion_cs_results:
                    st.json(completion_cs_results)
                elif completion_cs_results:
                    st.error(f"Completion CS Error: {completion_cs_results.get('error')}")

            else:
                st.warning("Skipping completion content safety analysis due to LLM error or empty completion.")
                st.session_state.results["completion_cs_analysis"] = {"status": "skipped"}


            # 5. Log everything
            final_log_payload = {
                "timestamp": request_timestamp,
                "user_prompt": llm_prompt,
                "llm_completion": llm_completion,
                "llm_model_name": AI_FOUNDRY_DEPLOYMENT_NAME,
                "llm_raw_response_summary": llm_full_response[:1000] if llm_full_response else None, # Log a summary
                "prompt_cs_analysis": st.session_state.results.get("prompt_cs_analysis"),
                "prompt_shield_analysis": st.session_state.results.get("prompt_shield_analysis"),
                "completion_cs_analysis": st.session_state.results.get("completion_cs_analysis"),
                "options_used": {
                    "blocklist_enabled": use_blocklist,
                    "selected_blocklist": selected_blocklist if use_blocklist else "N/A",
                    "prompt_shield_enabled": use_prompt_shield
                }
            }
            log_to_app_insights("AICoordinatorTestRun", final_log_payload)
            st.success("All operations complete. Results logged.")


with col2:
    st.header("üìä Results & Logs")
    if 'results' in st.session_state and st.session_state.results:
        st.subheader("Aggregated Run Data (for logging)")
        st.json(st.session_state.results, expanded=False)

        if "prompt_cs_analysis" in st.session_state.results:
            st.subheader("Prompt Content Safety Details")
            df_prompt_cs = pd.DataFrame(st.session_state.results["prompt_cs_analysis"].get("categories_analysis", []))
            if not df_prompt_cs.empty:
                st.table(df_prompt_cs)
            if st.session_state.results["prompt_cs_analysis"].get("blocklists_match"):
                st.write("Prompt Blocklist Hits:")
                st.json(st.session_state.results["prompt_cs_analysis"]["blocklists_match"])

        if "prompt_shield_analysis" in st.session_state.results:
            st.subheader("Prompt Shield Details (Conceptual)")
            st.json(st.session_state.results["prompt_shield_analysis"])


        if "completion_cs_analysis" in st.session_state.results and "error" not in st.session_state.results["completion_cs_analysis"]:
            st.subheader("Completion Content Safety Details")
            df_completion_cs = pd.DataFrame(st.session_state.results["completion_cs_analysis"].get("categories_analysis", []))
            if not df_completion_cs.empty:
                st.table(df_completion_cs)
            if st.session_state.results["completion_cs_analysis"].get("blocklists_match"):
                st.write("Completion Blocklist Hits:")
                st.json(st.session_state.results["completion_cs_analysis"]["blocklists_match"])
    else:
        st.info("Run an analysis to see results here.")

st.markdown("---")
st.caption("Ensure all necessary Azure resources are configured and environment variables are set.")
